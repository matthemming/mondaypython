{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "import re\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### going into archive years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlarch = 'http://www.northyorksfire.gov.uk/news-events/breaking-news/Archive'\n",
    "req = request.Request(urlarch, headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36'})\n",
    "content = request.urlopen(req)\n",
    "soup = BeautifulSoup(content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urldate = []\n",
    "for i in soup.find_all('a', {'title': re.compile(r'Archived news from')}):\n",
    "    urladd = i.get('href')\n",
    "    urldate.append(urladd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlyearlist = []\n",
    "for i in urldate:\n",
    "    archive_urls = 'http://www.northyorksfire.gov.uk{}/-/page/'.format(i)\n",
    "    urlyearlist.append(archive_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.northyorksfire.gov.uk/news-events/breaking-news/Archive/-/year/2018/-/page/',\n",
       " 'http://www.northyorksfire.gov.uk/news-events/breaking-news/Archive/-/year/2017/-/page/',\n",
       " 'http://www.northyorksfire.gov.uk/news-events/breaking-news/Archive/-/year/2016/-/page/',\n",
       " 'http://www.northyorksfire.gov.uk/news-events/breaking-news/Archive/-/year/2015/-/page/']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlyearlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### breaking news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.northyorksfire.gov.uk/news-events/breaking-news/-/page/1/',\n",
       " 'http://www.northyorksfire.gov.uk/news-events/breaking-news/-/page/2/',\n",
       " 'http://www.northyorksfire.gov.uk/news-events/breaking-news/-/page/3/',\n",
       " 'http://www.northyorksfire.gov.uk/news-events/breaking-news/-/page/4/',\n",
       " 'http://www.northyorksfire.gov.uk/news-events/breaking-news/-/page/5/',\n",
       " 'http://www.northyorksfire.gov.uk/news-events/breaking-news/-/page/6/',\n",
       " 'http://www.northyorksfire.gov.uk/news-events/breaking-news/-/page/7/',\n",
       " 'http://www.northyorksfire.gov.uk/news-events/breaking-news/-/page/8/',\n",
       " 'http://www.northyorksfire.gov.uk/news-events/breaking-news/-/page/9/',\n",
       " 'http://www.northyorksfire.gov.uk/news-events/breaking-news/-/page/10/',\n",
       " 'http://www.northyorksfire.gov.uk/news-events/breaking-news/-/page/11/',\n",
       " 'http://www.northyorksfire.gov.uk/news-events/breaking-news/-/page/12/']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# gets the urls for the most recent 2-3 months of incident summaries\n",
    "\n",
    "newpages = []\n",
    "breakingurl = 'http://www.northyorksfire.gov.uk/news-events/breaking-news/-/page/'\n",
    "page_count = 1\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        urlnewpage = breakingurl + str(page_count) + '/'\n",
    "        req = request.Request(urlnewpage, headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36'})\n",
    "        content = request.urlopen(req)\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        page_count += 1\n",
    "    except:\n",
    "        page_count = 1\n",
    "        break\n",
    "    if soup.find_all('a', {'title': re.compile(r'Incident Summary')}) == []:\n",
    "        page_count = 1\n",
    "        break\n",
    "    newpages.append(urlnewpage)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### going through pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the urls for the archived incident summaries\n",
    "\n",
    "archivepages = []\n",
    "\n",
    "page_count = 1\n",
    "\n",
    "for year_url in urlyearlist:\n",
    "    while True:\n",
    "        try:\n",
    "            urlpage = year_url + str(page_count) + '/'\n",
    "            req = request.Request(urlpage, headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36'})\n",
    "            content = request.urlopen(req)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            page_count += 1\n",
    "        except:\n",
    "            page_count = 1\n",
    "            break\n",
    "        if soup.find_all('a', {'title': re.compile(r'Incident Summary')}) == []:\n",
    "            page_count = 1\n",
    "            break\n",
    "        archivepages.append(urlpage)\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpages = newpages + archivepages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### going into incident on a page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the urls for the incident summary pages from the breaking news aand archive pages\n",
    "\n",
    "urlincident = []\n",
    "for page in allpages:\n",
    "    req = request.Request(page, headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36'})\n",
    "    content = request.urlopen(req)\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    for i in soup.find_all('a', {'title': re.compile(r'Incident Summary')}):\n",
    "        urladd = i.get('href')\n",
    "        urlincident.append('http://www.northyorksfire.gov.uk{}'.format(urladd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3550"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urlincident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 2 of each url on the pages so this drops the duplicates\n",
    "\n",
    "fixedurllist = list(set(urlincident))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1775"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fixedurllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.northyorksfire.gov.uk/news-events/breaking-news/incident_summary_night_shift_friday_29th_september_2018'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixedurllist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1775/1775\n"
     ]
    }
   ],
   "source": [
    "# scrapes each paragraph from the incident summary pages\n",
    "\n",
    "textlines = []\n",
    "count = 1\n",
    "for url in fixedurllist:\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    dayreq = request.Request(url, headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36'})\n",
    "    daycontent = request.urlopen(dayreq)\n",
    "    daysoup = BeautifulSoup(daycontent, 'html.parser')\n",
    "    print (str(count) + '/' + str(len(fixedurllist)))\n",
    "    count += 1\n",
    "    \n",
    "    textlines = textlines + [line.text for line in daysoup.find('div', id='col2').findAll('p')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleantextlines = [i.replace('\\xa0', '') for i in textlines if (i != '\\xa0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = pd.DataFrame(cleantextlines)\n",
    "textdf = textdf.rename(index=str, columns={0: 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf.to_csv(r'C:\\git\\fire\\data\\Fire_Regions\\NorthYorkshireFire.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
